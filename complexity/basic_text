[slide 3]
1.современные модели глубокого обучения обладают огромным количеством параметров
2.Неэффективное обучение 
3.Для порождающих моделей не выполняется условие на количеством параметов
4.Ситуация усугубляется тем, что модель имеет структурную сложность

[slide 4]
1.MDL - это не метод, это скорее принцип
2.Пример - побеждает вторая модель, имеющая хорошее отношение 


[slide 5]
1.MDL непосредственно связан с Колмогоровской сложностью

[slide 6]
1. в случае, если нам доступна какая-то вероятностная интерпретация модели, то одной из наиболее известных интерпретаций MDL является оптимальная универсальная модель MDL.
2.С точки зрения этой интепретации в качестве длины описания выборки выступает правдоподобие модели, а в качестве второго слагаемого, отвечающего за длину описания модели --- стохастическая сложность COMP, являющаяся суммой правдоподобий по всем возможным выборкам. 
3. Минус - честно оценку MDL lля модели можно вычислить только в случае конечности множества выборок
4.Есть схожая байесовская интерпретация
[slide 7]
1.Как соотносится с MDL

[slide 8]
1.prior - с одной стороны позволяет выбирать хорошие признаки
с другой, много произвола
2.проблема Байеса

[slide 9]
Отличия основные

[slide 10]
LOU никогда не оценивает выборку целиком. Понятно, что если мы используем не LOU, а k-fold кросс-валидацию, то оценка сложности будет только смещаться.

[slide 11]
1.Laplace - физическая интерпретация
2.MC - технически сложно реализовать. Большинство методов требуют оценки по всей выборки.

[slide 12]
1.KL - не между неизвестным распределением, а прайором
2.MDL 
