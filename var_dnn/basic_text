[slide 2]
1.современные модели глубокого обучения обладают огромным количеством параметров
2.Неэффективное обучение 
3.Для порождающих моделей не выполняется условие на количеством параметов
4.Ситуация усугубляется тем, что модель имеет структурную сложность


[slide 3]
1. Реальный пример
2. Авторы решают задачу поиску моделей с использованием обучения с подкреплением
3. Виден разрыв в числе параметров
4. Мобильные технологии
5. Задача компрессии


[slide 4]
1.MDL - это не метод, это скорее принцип
2.Пример - побеждает вторая модель, имеющая хорошее отношение 

[slide 5]
1.MDL непосредственно связан с Колмогоровской сложностью

[slide 6]
1. в случае, если нам доступна какая-то вероятностная интерпретация модели, то одной из наиболее известных интерпретаций MDL является оптимальная универсальная модель MDL.
2.С точки зрения этой интепретации в качестве длины описания выборки выступает правдоподобие модели, а в качестве второго слагаемого, отвечающего за длину описания модели --- стохастическая сложность COMP, являющаяся суммой правдоподобий по всем возможным выборкам. 
3. Минус - честно оценку MDL lля модели можно вычислить только в случае конечности множества выборок
4.Есть схожая байесовская интерпретация

[slide 7]
1. Что такое
2. Левый график
3. Правый график

[slide 8]
1.prior - с одной стороны позволяет выбирать хорошие признаки
с другой, много произвола
2.проблема Байеса

[slide 9]
LOU никогда не оценивает выборку целиком. Понятно, что если мы используем не LOU, а k-fold кросс-валидацию, то оценка сложности будет только смещаться.

[slide 10]
В дальнейшем будем называть оптимальной моделю ту модель, которая дает наилучшее правдоподобие

[slide 11]
Приведем пример множества моделей, для которых можно найти оптимальную

[slide 12]
1.Laplace - физическая интерпретация

[slide 13]
1.MC - технически сложно реализовать. Большинство методов требуют оценки по всей выборки.
2. В то же время, MC часто используется как подметод. Например, в вар. выводе, о котором дальше пойдет речь.


[slide 14]
В чем заключается метод.
Пример слева: как работает.
Пример справа: как работает в сравнении с Лапласом.

[slide 15]
1. Прокомментировать выкладку
2. KL между прайором, а не  неизвестным расспределением

[slide 16]
1. Прокомментировать выкладку
2. Субоптимальность. Заметка про разницу между D_KL

[slide 17]
Пояснение разницы между ассиметрией в D_KL


[slide 18]
1. Для чего
2. Почему?


[slide 19]
пояснить на пальцах

[slide 20]
пояснить на пальцах
плохо работает.

[slide 21]
Graves --- предложил использовать идею для нейросетей. 
В этой работе предлагается метод прунинга, основанный на соотношении плотностей.

[Slide 22]
Дальнейшее развистие идей.
Вместо того, чтобы оптимизировать гиперпараметры EM, мы рассматриваем их как параметры.
Группировка.

[slide 23]
Еще один пример CV vs. Evidence.

[slide 24]
Мотивация -- можно ли рассматривать SGD как вар. процесс

[slide 25-26]
Пояснить

[slide 27]
Заметитм, что энтропия всегда уменьшается.  
Это вполне логично

[slide 28]
Переобучение.
Что делать, если нам  хочется получить более устойчивый процесс?

[slide 29]
Заметим, что работает он хорошо в теории.
На практике, он достаточно капризный

[slide 30]
Пояснить

[slide 31]
1. Плато - в районе 0.6
2. Не дало максимум, но дало начало расхождения
3. Нормальное распределение дало лучший результат

[slide 32]
Возмущение параметров.
SGD дает заниженный результат на классификации.

    
 
