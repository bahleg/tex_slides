1. Добрый день! Сегодня я хотел бы рассказать о своей диссертационной работе, Байесовский выбор
субоптимальной структуры
модели глубокого обучения

2. Рассматривается задача выбора структуры модели глубокого обучения.Основные задачи --- это предложить критерии оптимальной и субоптимальной сложности модели, а также алгоритм построения модели субпотимальной сложности.  Основной проблемой при построении моделей глубокого обучения является многоэкстремальность и высокая вычислительная сложность оптимизации, т.к. количество параметров в моделях может исчилсться миллионами. Предлагаются методы, основанные на вариационном байесовском выводе и градиентной оптимизации гиперпараметров модели.

3. Одна из основных проблем моделей глубокого обучения - их заведомо избыточная сложность. На данном слайде представлены два графика для модели автокодировщика. Левый график показывает, что удаляя неинформативные или неревантные параметры можно добиться небольшого снижения качества при значимом снижении количества параметров. Правый график показывает, что модель с меньшим количеством параметров может быть более устойчиовй к шуму в выборке. Правильный выбор модели приводит как к синжению эксплуатационных затрат, так и к повышению качеств итоговой модели.

4. Моделью будем называть дифференцируемую по параметрам функцию из множества признаковых описаний во множество меток. Модель может состоять из набора суперпозиций. Этот набор и взаимосвзяь между ними определяется структорй модели. 

5. Рассмотрим пример: однослойную нейросеть. Задача выбора структуры состоит в выборе количества нейронов на скрытом слое нейронной сети. Пусть выбор осуществляется между двумя альтернативами. Каждая альтернатива задается обобщенно-линейной функцией со своей матрицей. Каждая матрица имеет заданное наперед количество ненулевых стоблцов. Это количество ненулевых стобцов и определяет количетсво нейронов на скрытом слое. Таким образом, задача выбора модели состоит в выборе одной из двух альтернатив. Бинарный вектор, определяющий влкад каждой альетернативы и модель назовем структурой нашей модели.

6. Пусть задан ациклический граф. Вершинами графа будут промежуточные состояния выборки, а ребрами - базовые функции над ними.\ Тогда граф со множеством заданных функций назовем параметрическим семейством.


7. Проиллюстрируем основные типы ограничений, которые можно ввести на структурные параметры гамма. Верхний левый график показывает случай, когда структурный параметр принадлежит булевому кубу, т.е. каждая компонента гамма принадлежит бинарному набору значений. Более интересному случаю соответствет нижний график. Здесь гамма принадлежит множеству вершин симплекса. В контексте предыдущего примера, про нейронную сеть с одним слоем данные ограничения дает нам возможность выбрать строго одну альтернативу, и.е. решить задачу по выбору количества нейронов на скрытом слое.


8. Предлагается рассматривать модель глубокого обучения как вероятностную модель. Введем  вероятностные предположения о распределении параметров. Параметры этого распределения, т.е. распределения самих параметров, назовем гиперпараметрами. Конкатенацию всех векторов гамма будем называть структурой модели. Метапараметры - это непосредственно параметры оптимизационной задачи. Они задаются экспертно и не подлежат оптимизации.


9. В качестве априорного распределения структуры модели предлагается использовать распределение гумблеь-сотфмакс. Его поведение при разны параметрах изображено в нижней части слайда. Данное распределено задано на симплесе и зависит от двух параметров - температуры и концентрации. Концентрация --- это вектор, задающий вклад каждой вершины симплекса в итоговое распределение. Температура позволяет Характеризует, насколько данное распределение будет сконцентрировано у вершин симплекса или в центре. Схожими характеристиками обладает распределение Дирихле. Показано, что при использовании градиентных методов оптимизации данное распределение сильно неустойчиво.


10. Перечислим характеристики предложенной вероятностной модели. Предлагаемая модель является обощением базовой модели. Основными отличиями являются: введение распределения на структуру модели, введение распределения на  на ковариационную матрицу параметров модели. Данное распределение выступает регуляризатором для матрицы А и штрафует оптимизацию в случае, если элементы матрицы будут принимать большие значение. Гиперпараметрами модели являются ковариационная матрица параметров А и параметр концентрации распределения Гумбель-софтмакс. Параметры распределения матрицы, а также параметр температуы яляются метапараметрами.

11. Выбор модели будем осуществлять согласно  принципе минимальной длины описания или MDL. Принцип заключается в том, что мы выбираем ту модель, чья длина описания в сумме с длиной описания выборки при условии этой модели, будет минимальна. Байесовской интерепретацией данного принципа является обоснованность модели  - интеграл правдоподобия выборки по всем возможным значениям параметров.  Пример интеграла обоснованности представлен на правом изображении. Выборка описывается тремя моделями. Отсюда видно, что интеграл штрафует модели плохо описывающие выборку, и переусложненные модели.


12.  Основная проблема выбора модели и структуры модели с использованием интеграла обоснованности заключается в том, что интеграл не вычислим аналитически. Для  вычисления  оценки интеграла правдоподоия использовать метод вариационной нижней оценки. Метод заключается в следующем. Введем некоторое распределение q, приближающее апостерирорное распределение параметров. Тогда справедлива нижняя оценка интеграла правдоподобия, полученная с помощью неравенства Йенсена. Задача нахождения значения интеграла правдоподобия сводится к задаче нахождения оптимальных значений параметров распределения q. 

13. Перейдем к формальной постановке задачи. Пусть задано вариационное распределение, приближающее апостериорное распределение параметров.
Пусть заданы две функции, функция валидации и функция потерь. Функция потерь отвечает за оптимизацию параметров, функция валидации отвечает за оптимизацию гиперпараметров модели. Задачей выбора модели назовем двухуровневую задачу оптимизации, которая выглядит следующим образом. На верхнем уровне мы оптимизируем гиперпараметры модели, при этом вариационные параметры модели оптимизируются на нижнем уровне и могут рассматриваться как неявная функция от гиперпараметров

14. введем понятие обобщающая задача. Обобщающей задачей будем называть оптимизационную задачу, которая охватывает достаточно большой класс критериев и задач оптимизации моделей. Сформулируем критерии, которым должна удовлетворять наша задача. На рассматриваемой нами области нижняя задча выбора параметров должна задаваться неявно как функция от гиперпараметров и структурных параметров. При некоторых значениях метапараметров задача должна: 
приближать метод максимального правдоподобия, штрафовать модели с исбыточным количеством параметров, максимизировать оценку обоснованности. Также для моделей с сильно различающимися структурами должна быть возможность переходить между ними при оптимизации. Последнее условие требуется для возможности непрерывного перехода между различными режимами оптимизации.

15. Утверждается, что следующие перечисленные критерии и методы выбора модели не являются обобщающими. Требуется предложить задачу оптимизации, которая обобщщила бы эти задачи.


16. Пердлагается задача оптимизации, являющаяся обобщающей. Верхнее и нижнее задачи состояит из слагамеых из вариацоинной оценки обоснованности. В верхнем слагаемом также добавлено слагаемое, позволяющее производить перебор структур. На верхнем графике представлен пример структур, получаемых с использованием данного слагамеого.  Видно, что таким образом можно получать различные структуры и осуществлять перебор.

17. При значениях метапараметров, представленных на слайдах, задача соответствует задаче нахождения параметров и структуры модели в соответствии с байесовским выводом. 
И вариационные распределения параметров приближают апостериорное распределение наилучшим образом.


18. Перейдем к методу решениюя рассматриваемой задачи оптимизации. Назовем оператором T выбор вектора параметров по параметрам предыдущего шага. Примером такого оператора может послужить оператор градиентного спуска. Утверждается, что если выполнен ряд условий, то задачу оптимизации можно переписать следующим образом. Полученная задача будет непрерывной по метапараметрам модели.

19. Перепишем вариационной оценки обоснованности. Пусть наше вариационное распределение эмпирическими описывается параметрами, полученными под действием оператора оптимизации. Для получения вариационной оценки на основе такого распределения была сформулирована данная теорема.

20. Если рассматривать в качестве оператора оптимизации градиентный спуск, то такая оптимизация не оптимизирует оценку обоснованности. Поэтому начиная с какого-то момента оптимизация будет снижать оценку обоснованности, что говорит о начале переобучения. Таким образом в качестве вариационного распределения параметров можно брать не какое-то фиксированное распределение, а просто оптимизировать параметры в стандартной процедуре. При этом есть возможность получить оценку начала переобучения без использования кросс-валидации.

22.  Рассмотрим поведения модели при различных значениях метапараметров. Доказано, что в ассимптотике калибровка коэффициента перед дивергенций кульбака лейблера позволяет производить оптимизацию вариационной оценки для выборок разного эффективного размера. Параметрической сложностью модели назовем миним альную дивергенцию между вариационным и априиорным распределением. Если параметр хорошо описывается априорным распределением, то в целом он не несет никакой полезной информации для модели и его часто можно заменить на моду распределения. Снижение параметрической сложности эквивалентно увеличинию относительной плотности параметров в нуле. Относительная плотность является одним из критериев удваления неинформативных параметров.

23.Здесь приведены еще две теоремы. Первая говорит о том, что при устремлении к бесконечности коэжффициента перед дифергецнией кульбака лейблера параметрическая сложность снижается. Вторая теорема дает оценку разности параметрических сложностей для двух моделей, полученных с различными значениями этого коэффициента. В частности, при выполнении ряда условий, модель, прооптимизированная с большим значением этого коэффициента будет иметь меньшую параметрическую сложность.


убрать указания.
Бинарные веса
третий слайд - проглотил.
