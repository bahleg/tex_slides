\documentclass[11pt,pdf,utf8,russian,aspectratio=169]{beamer}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{subfig}
\usepackage{color}
\usepackage{multicol}
\usepackage{appendixnumberbeamer}
\usepackage{multicol}

\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator*{\argmax}{arg\,max}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Boadilla}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{seagull} % or try albatross, beaver, crane, ..

  \usefonttheme{structurebold}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 


\captionsetup[subfloat]{labelformat=empty}
\title[Последовательное порождение моделей]{Последовательное порождение моделей \\глубокого обучения оптимальной сложности}
\subtitle
{\small Диссертация на соискание ученой степени \\
кандидата физико-математических наук \\
05.13.17~--- Теоретические основы информатики\\
научный руководитель д.ф.-м.н В.\,В.\,Стрижов}
\author{О.\,Ю.\,Бахтеев}

\institute[МФТИ]{Московский Физико-Технический Институт (Государственный Университет)}     
%\institute[МФТИ]{Московский Физико-Технический Институт (Государственный Университет)}
\date[99.99.2019]{99 июбря 2019 г.}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Задача выбора модели}
\textbf{Цели исследования}\\
\begin{enumerate}
\item Предложить критерии оптимальной и субоптимальной сложности модели глубокого обучения.
\item Предложить метод построения модели субоптимальной сложности.
\end{enumerate}
\textbf{Задачи}\\
\begin{enumerate}
\item Предложить критерий оптимальной и субоптимальной сложности модели глубокого обучения.
\item Разработать алгоритм построения модели глубокого обучения субоптимальной сложности.
\item Предложить методы оптимизации параметров и гиперпараметров модели.
\item Предложить обобщенный метод выбора модели глубокого обучения.
\item Разработать программный комплекс для построения моделей глубокого обучения для задач классификации и регрессии.
\end{enumerate}
\end{frame}


\begin{frame}                                                                                                                                   
\frametitle{Проблемы обучения сетей}                                                                                                            
Правдоподобие моделей с избыточным количеством параметров не меняется при удалении параметров.                                                       
\begin{figure}[h]                                                                                                                               
\centering                                                                                                                                      
\subfloat[Избыточность параметров модели]{\includegraphics[width=0.45\textwidth]{./slide_plots/pruning.pdf}}                                          
\subfloat[Неустойчивость модели]{\includegraphics[width=0.45\textwidth]{./slide_plots/noise.pdf}}                                                     
\end{figure}                                                                                                                                    
                                                                                                                                                
\end{frame}    

\begin{frame}{Зависимость правдоподобия от гиперпараметров}
           
\begin{figure}[h]                                                                                                                               
\centering  
   \includegraphics[width=0.5\textwidth]{./slide_plots/hyper.png}
\end{figure}
\end{frame}


\begin{frame}{Формальная постановка задачи}


\end{frame}


\begin{frame}{L и Q: Кросс-валидация}
Разобьем выборку $\mathfrak{D}$ на $k$ равных частей:
\[
\mathfrak{D} = \mathfrak{D}_1 \sqcup \dots \sqcup \mathfrak{D}_k.
\]

Запустим $k$ оптимизаций модели, $r$-я модель обучается на выборках $\mathfrak{D}^r = \mathfrak{D}_1,\dots,\mathfrak{D}_{r-1},\mathfrak{D}_{r+1},\dots,\mathfrak{D}_k$.

Положим $\boldsymbol{\theta} = [\mathbf{w}_1, \dots, \mathbf{w}_k]$ --- параметры всех запусков модели.
\[
L(\boldsymbol{\theta}, \mathbf{A}^{-1}, \mathfrak{D}) = -\frac{1}{k}\sum_{r=1}^k \bigl(\frac{k}{k-1}\textcolor{blue}{\text{log}p(\mathfrak{D}^r|\mathbf{w}_r)} - \textcolor{red}{\text{log}p(\mathbf{w}_r|\mathbf{A}^{-1})}\bigr).
\]

\[
Q(\boldsymbol{\theta}, \mathbf{A}^{-1}, \mathfrak{D}) = \frac{1}{k}\sum_{r=1}^k k\textcolor{blue}{\text{log}p(\mathfrak{D}_r|\mathbf{w}_r)}.
\]


\end{frame}

\begin{frame}       
\begin{multicols}{2}
                                                                                                                            
\frametitle{Правдоподобие модели}                                                                                                         
                                                                                                                       
Модель $\mathbf{f} \in \mathfrak{F}$  оптимальна, если достигается максимум \textbf{правдоподобия модели}:                                      
\[                                                                                                                                              
        p(\mathfrak{D}|\mathbf{A}^{-1}) = \int_\mathbf{w} \textcolor{blue}{p( \mathfrak{D}| \mathbf{w})}\textcolor{red}{p(\mathbf{w}|\mathbf{A}^{-1})}d\mathbf{w}.                           
\]                                                                                                                                              
                                                                                                                                                
\begin{figure}[h]                                                                                                                               
\includegraphics[width=0.45\textwidth]{./slide_plots/example.pdf}                                                                                      
\end{figure}                                                                                                                                    
\columnbreak
Пусть $q$ --- непрерывное распределение.
$$                                                                                                                                              
        \text{log}~p(\mathfrak{D}|\mathbf{A}^{-1}) \geq \int q(\mathbf{w})\text{log}~\frac{p(\mathfrak{D},\mathbf{w}|\mathbf{A}^{-1})}{q(\mathbf{w})}d\mathbf{w} =                                                                                                                                        
$$           
$$                                                                                                                                              
        = \textcolor{blue}{\int q(\mathbf{w})\text{log}~{p(\mathfrak{D} | \mathbf{w},\mathbf{A}^{-1})}d \mathbf{w}}  - \textcolor{red}{\text{D}_{\text{KL}}
} ,                                                                                                                                
$$      
где $$\textcolor{red}{\text{D}_{\text{KL}}} = -\int q(\mathbf{w})\text{log}~\frac{p(\mathbf{w} |\mathbf{A}^{-1})}{q(\mathbf{w})}d\mathbf{w}.$$ 

\end{multicols}
\end{frame}                                                                                                                                     
              
\begin{frame}
\frametitle{Вариационная оценка на основе мультистарта}
$$\text{log}p(\mathfrak{D}|\mathbf{A}) \geq \mathsf{E}_{q(\mathbf{w)}}[\text{log~}p (\mathfrak{D}, \mathbf{w}| \mathbf{A}^{-1})] - \mathsf{S}({q(\mathbf{w)}}),$$
$\mathsf{S}$ --- энтропия.

\textbf{Теорема [Бахтеев, 2016].}~Пусть $L$ --- функция потерь, градиент которой ---  непрерывно-дифференцируемая функция с константой Липшица $C$. Пусть $\boldsymbol{\theta} = [\mathbf{w}^1,\dots,\mathbf{w}^k]$ ---  начальные приближения оптимизации модели. Пусть $\gamma$ --- шаг градиентного спуска, такой что:
\begin{itemize}
\item $\gamma<\frac{1}{C}$,
\item $\gamma^{(-1)} > \max_{r \in \{1,\dots,k\}}\lambda_\text{max} (\mathbf{H}(\mathbf{w}^r))$.
\end{itemize}
Тогда
\small
\[
	\mathsf{S}(q^\tau(\mathbf{w})) -  \mathsf{S}(q^{\tau-1}(\mathbf{w}))  \sim  \frac{1}{k}\sum_{r=1}^k \bigl(\gamma Tr[\mathbf{H}(\mathbf{w}^r)] - \gamma^2 Tr[\mathbf{H}(\mathbf{w}^r)\mathbf{H}(\mathbf{w}^r)]  \bigr) + o_{\gamma \to 0}(1),
\]
где $\mathbf{H}$ --- гессиан функции потерь $L$, $q^\tau$ --- распределение $q(\mathbf{w})$ в момент оптимизации $\tau$.
\end{frame}


\begin{frame}
\frametitle{\smallВариационная оценка с использованием градиентного спуска}
Максимизация вариационной оценки эквивалентна минимизации $\text{D}_{\text{KL}}(q(\mathbf{w})||p(\mathbf{w} | \mathfrak{D},\mathbf{A}^{-1}))$.
Градиентный спуск не минимизирует $\text{D}_{\text{KL}}(q(\mathbf{w})||p(\mathbf{w} | \mathfrak{D},\mathbf{A}^{-1}))$.
\begin{multicols}{2}

\begin{figure}
\subfloat{\includegraphics[width=0.42\textwidth]{./slide_plots/sgd_estimate.png}}
\end{figure}

\columnbreak


\begin{figure}
{\includegraphics[width=0.42\textwidth]{./slide_plots/sgd_show.pdf}}
\end{figure}
\end{multicols}
\end{frame}

                                                                                                                                  
                                                                                                                                                
                                                                                                                                               
\iffalse                                                                                                                                                
\begin{frame}                                                                                                                                   
\frametitle{Вариационная оценка интегральной функции правдоподобия}                                                                             
\textbf{Проблема:} вычисление оценки правдоподобия модели имеет высокую вычислительную сложность.                                               
\small                                                                                                                                          
    
\textbf{Утверждение [Bishop, 2006].} Справедливы нижние оценки интегральной функции правдоподобия:                                              
$$                                                                                                                                              
        \text{log}~p(\mathfrak{D}|\mathbf{A}^{-1}) \geq \int q(\mathbf{w})\text{log}~\frac{p(\mathfrak{D},\mathbf{w}|\mathbf{A}^{-1})}{q(\mathbf{w})}d\mathbf{w} =                                                                                                                                        
$$           
$$                                                                                                                                              
        = -\text{D}_{\text{KL}} (q(\mathbf{w})||p(\mathbf{w}|\mathbf{A}^{-1})) + \int q(\mathbf{w})\text{log}~{p(\mathfrak{D} | \mathbf{w},\mathbf{A}^{-1}
)} d \mathbf{w},                                                                                                                                
$$      
где $$\text{D}_{\text{KL}}(q(\mathbf{w})||p(\mathbf{w} | \mathbf{A}^{-1})) = -\int q(\mathbf{w})\text{log}~\frac{p(\mathbf{w} |\mathbf{A}^{-1})}{q(\mathbf{w})}d\mathbf{w},$$ $q \in Q$ --- параметрическое семейство распределений.                                                                     
     

\end{frame} 
\fi           

\begin{frame}{L и Q: Вариационная оценка}
Пусть $q = \mathcal{N}(\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q), \quad \boldsymbol{\theta} =  [\boldsymbol{\mu}_q, \mathbf{A}^{-1}_q].$ \\
Тогда вариационная оценка имеет вид:
$$
\textcolor{blue}{\int_{\mathbf{w}} q(\mathbf{w})\text{log}~{p(\mathfrak{D},\mathbf{w},\mathbf{A}^{-1})} d \mathbf{w}} - \textcolor{red}{D_\text{KL}\bigl(q (\mathbf{w} )|| p (\mathbf{w}|\mathbf{A}^{-1})\bigr)} \simeq
$$
$$
\sum_{i=1}^m \textcolor{blue}{\text{log}~p(\mathbf{x}_i | \mathbf{w}_i)} - \textcolor{red}{D_\text{KL}\bigl(q (\mathbf{w} )|| p (\mathbf{w}|\mathbf{A}^{-1})\bigr)} = -L(\boldsymbol{\theta}, \mathbf{A}^{-1}, \mathfrak{D}) = Q(\boldsymbol{\theta}, \mathbf{A}^{-1}, \mathfrak{D}),
$$
где $\mathbf{w}_i \sim q$.

Дивергенция $\textcolor{red}{D_\text{KL}\bigl(q (\mathbf{w} )|| p (\mathbf{w}|\mathbf{A}^{-1})\bigr)}$ вычисляется аналитически:
$$
\textcolor{red}{D_\text{KL}\bigl(q (\mathbf{w}) || p (\mathbf{w}|\mathbf{A}^{-1})\bigr)} = \frac{1}{2} \bigl( \text{tr} (\mathbf{A}\mathbf{A}^{-1}_q) + \boldsymbol{\mu}_q^\text{T}\mathbf{A}\boldsymbol{\mu}_q - n +\text{ln}~|\mathbf{A}^{-1}| - \text{ln}~|\mathbf{A}^{-1}_q| \bigr).
$$


\end{frame}

\begin{frame}{Общая схема алгоритма оптимизации}
\textbf{Вход:} количество итераций оптимизации гиперпараметров $\ell$, количество итераций оптимизации параметров $\tau$, 
длина шага градиентного спуска $\gamma$.

\begin{enumerate}
\item Повторять в цикле от $1,\dots, \ell$:
\begin{enumerate}
\item Инициализировать параметры $\boldsymbol{\theta}_0$.
\item Провести оптимизацию параметров с использованием стохастического градиентного спуска:
\[
    \hat{\boldsymbol{\theta}} = T \circ T \circ \dots \circ T(\boldsymbol{\theta}_0) = T^\tau(\boldsymbol{\theta}_0),
\]
где 
\[
    T(\boldsymbol{\theta}) =\boldsymbol{\theta} - \gamma \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \mathbf{A}^{-1}, \hat{\mathfrak{D}}),
\]
$\mathfrak{D}$ --- случайная подвыборка $\mathfrak{D}$.
\item Провести оптимизацию гиперпараметров: $Q(\hat{\boldsymbol{\theta}}(\mathbf{A}^{-1}), \mathbf{A}^{-1}, \mathfrak{D}) \to \max$.

\end{enumerate}
\end{enumerate}
\end{frame}

\begin{frame}{HOAG и Жадный алгоритм}
\begin{multicols}{2}
\begin{figure}[h]
\includegraphics[width=0.45\textwidth]{./slide_plots/greed_hoag.png}
\end{figure}
\columnbreak
\textbf{Жадный алгоритм}
\[
	\mathbf{A}'^{-1} = \mathbf{A}^{-1} - \gamma_{\mathbf{A}} (\nabla_{\mathbf{A}^{-1}}  Q (T(\boldsymbol{\theta}) , \mathbf{A}^{-1}, \mathfrak{D})), %=  \mathbf{A}^{-1} - \gamma_{\mathbf{A}} (\nabla_{\mathbf{A}^{-1}}  Q(\boldsymbol{\theta} - \gamma \nabla L(\boldsymbol{\theta}, \mathbf{A}^{-1}), \mathbf{A}^{-1}, \mathfrak{D})),
\]
где $\gamma_{\mathbf{A}}$ --- длина шага оптимизации гиперпараметров.
\\~\\
\textbf{HOAG}
\[
\mathbf{A}'^{-1} = \mathbf{A}^{-1} - \gamma_{\mathbf{A}} \hat{\nabla}_{\mathbf{A}^{-1}} Q(T^\tau(\boldsymbol{\theta}_0),  \mathbf{A}^{-1}, \mathfrak{D})),
\]
где $\hat{\nabla}_{\mathbf{A}^{-1}}$ --- численное приближение градиента:
\[
%{\nabla}_{\mathbf{A}^{-1}} Q(T^\tau(\boldsymbol{\theta}_0), \mathbf{A}^{-1}, \mathfrak{D})) = 
\nabla_{\mathbf{A}^{-1}} Q(\hat{\boldsymbol{\theta}}) - (\frac{\partial^2{L}}{\partial \boldsymbol{\theta} \partial \mathbf{A}^{-1}})^\text{T} \mathbf{H}(L({\boldsymbol{\theta}}))^{-1} \nabla_{\boldsymbol{\theta}} Q.
\]
где $\mathbf{H}$ --- гессиан.
\end{multicols}
\end{frame}

\begin{frame}{Алгоритм DrMad}
\begin{multicols}{2}
\begin{figure}[h]
\includegraphics[width=0.45\textwidth]{./slide_plots/mad.png}
\end{figure}


Рассматривается оптимизация функции $Q(T^\tau(\boldsymbol{\theta}_0) , \mathbf{A}^{-1}, \mathfrak{D}))$ по \textbf{всей} истории оптимизации параметров.\\~\\
Вводятся предположения о линейности траектории оптимизации параметров. \\

Градиент $\nabla_\mathbf{A}^{-1} Q$ аккумулируется по правилу:
$$\nabla_{\mathbf{A}^{-1}} Q= \nabla_{\mathbf{A}^{-1}} Q - \gamma \nabla_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}^\tau) \nabla_{\mathbf{A}^{-1}}  \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^\tau).
$$
\end{multicols}

\end{frame}

\begin{frame}{Вычислительный эксперимент}
\textbf{Цель эксперимента:} анализ рассматриваемых алгоритмов и итоговых моделей.

\textbf{Данные:}
\begin{itemize}
\item Синтетические данные: 40 точек на плоскости. Модель: полином 12 степени.
\item Набор записей акселерометра WISDM. Рассматривается задача регрессии с нейронной сетью с одним скрытым слоем (10 нейронов).
\item Набор рукописных цифр MNIST. Рассматривается задача регрессии с нейронной сетью с одним скрытым слоем (300 нейронов).
\end{itemize}

В качестве Q и L рассматривается кросс-валидация ($k=4$) и вариационная оценка.
\end{frame}

\begin{frame}{Синтетические данные: результат}
\begin{multicols}{2}
\begin{figure}[h]
\includegraphics[width=0.4\textwidth]{./slide_plots/poly_cv.png}
\caption*{Кросс-Валидация}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.4\textwidth]{./slide_plots/poly_var.png}
\caption*{Вариационная оценка}
\end{figure}
\end{multicols}

\end{frame}


\begin{frame}{WISDM: кросс-валидация}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/wisdm_q_cv.png}
\end{figure}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/wisdm_e_cv.png}
\end{figure}
\end{frame}

\begin{frame}{WISDM: вариационная оценка}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/wisdm_q_var.png}
\end{figure}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/wisdm_e_var.png}
\end{figure}
\end{frame}

\begin{frame}{MNIST: кросс-валидация}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/mnist_q_cv.png}
\end{figure}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/mnist_e_cv.png}
\end{figure}
\end{frame}

\begin{frame}{MNIST: вариационная оценка}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/mnist_q_var.png}
\end{figure}
\begin{figure}[h]
\includegraphics[width=\textwidth]{./slide_plots/mnist_e_var.png}
\end{figure}
\end{frame}


\begin{frame}{MNIST: добавление шума}
Добавление гауссового шума $\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})$:
\setlength{\columnsep}{10pt}
\begin{multicols}{4}
\begin{figure}[h]
\includegraphics[width=0.10\textwidth]{./slide_plots/mnist0.png}
\caption*{Без шума}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.10\textwidth]{./slide_plots/mnist10.png}
\caption*{$\sigma=0.1$}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.10\textwidth]{./slide_plots/mnist25.png}
\caption*{$\sigma=0.25$}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.10\textwidth]{./slide_plots/mnist50.png}
\caption*{$\sigma=0.5$}
\end{figure}
\end{multicols}
\begin{center}
\includegraphics[width=0.9\textwidth]{./slide_plots/noise.png}
\end{center}
\end{frame}


\begin{frame}{Результаты, выносимые на защиту}
\begin{itemize}
\item Предложен метод критерий и субоптимальной сложности модели глубокого обучения.
\item Разработан алгоритм построения модели глубокого обучения субоптимальной сложности.
\item Предложены методы оптимизации параметров и гиперпараметров модели.
\item Предложен обобщенный метод выбора модели глубокого обучения.
\item Разработан программный комплекс для построения моделей глубокого обучения для задач классификации и регрессии.
\end{itemize}
\end{frame}



\begin{frame}[plain, noframenumbering]{Список работ автора по теме диссертации}
\footnotesize
\textbf{Публикации ВАК}\\
\begin{enumerate}
\item А.Н. Смердов,  О.Ю. Бахтеев, В.В. Стрижов, Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза // Информатика и её применения, 2019
\item О.Ю. Бахтеев, В.В. Стрижов, Выбор моделей глубокого обучения субоптимальной сложности // Автоматика и телемеханника, 2018
\item Бахтеев О.Ю., Попова М.С., Стрижов В.В., Системы и средства глубокого обучения в задачах классификации // Системы и средства информатики, 2016, 26(2) : 4-22
\item \textbf{Еще несколько работ из АП, нужно вставлять?}
\end{enumerate}
\textbf{Выступления с докладом}\\
\begin{enumerate}
\item Градиентные методы оптимизации гиперпараметров моделей глубокого обучения, Математические методы распознавания образов, 2017
\item Выбор модели глубокого обучения субоптимальной сложности с использованием вариационной оценки правдоподобия, Интеллектуализация обработки информации 2016.
\item \textbf{Еще несколько работ из АП, нужно вставлять?}
\end{enumerate}
\end{frame}


\end{document}
