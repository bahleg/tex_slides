{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Байесовский выбор субоптимальной структуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "%matplotlib inline\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32*32*1 \n",
    "class_num = 10\n",
    "n_epochs =  20\n",
    "fine_tune_epochs = 10\n",
    "batch_size = 256\n",
    "random_seed = 42\n",
    "valid_size = 0.1 # валидация не используется. Сохраняем ее для чистоты эксперимента\n",
    "trials = 10\n",
    "search_space = [1, 16, 32, 64, 256, 512, 1024]  # '1' кодирует тождественное отображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10('./files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.mean(0).view(-1))\n",
    "                             ]))\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10('./files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                              (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.mean(0).view(-1))\n",
    "                             ]))\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True )\n",
    "test_loader = t.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "valid_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sigma = -3.0\n",
    "init_h_sigma = 0.0\n",
    "class VarLayer(nn.Module):\n",
    "    def __init__(self, in_, out_, device='cuda',  act=F.tanh):\n",
    "        nn.Module.__init__(self)        \n",
    "        self.mean = nn.Parameter(t.randn(in_, out_).cuda())\n",
    "        t.nn.init.xavier_uniform(self.mean)\n",
    "        self.log_sigma = nn.Parameter(t.ones(in_, out_).cuda()*(init_sigma))    \n",
    "        self.mean_b = nn.Parameter(t.randn(out_).cuda())\n",
    "        self.log_sigma_b = nn.Parameter(t.ones(out_).cuda()*(init_sigma))\n",
    "        \n",
    "        \n",
    "        self.h_sigma = nn.Parameter(t.ones(in_, out_).cuda()*(init_h_sigma))    \n",
    "        self.h_sigma_b = nn.Parameter(t.ones(out_).cuda()*(init_h_sigma))   \n",
    "         \n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,x, gamma=1.0):\n",
    "        if self.training:\n",
    "            self.eps_w = t.distributions.Normal(self.mean, t.exp(self.log_sigma))\n",
    "            self.eps_b = t.distributions.Normal(self.mean_b, t.exp(self.log_sigma_b))\n",
    "        \n",
    "            w = self.eps_w.rsample()\n",
    "            b = self.eps_b.rsample()\n",
    "        else:            \n",
    "            w = self.mean\n",
    "            b = self.mean_b\n",
    "        return self.act(t.matmul(x, w)+b)*gamma\n",
    "\n",
    "    def KLD(self, weights = 1.0, weights_b = 1.0):\n",
    "        self.eps_w = t.distributions.Normal(self.mean * weights, t.exp(self.log_sigma) *weights)\n",
    "        self.eps_b = t.distributions.Normal(self.mean_b * weights_b, t.exp(self.log_sigma_b) * weights_b)\n",
    "        self.h_w = t.distributions.Normal(t.zeros(self.mean.size()).cuda(), t.exp(self.h_sigma)*weights)\n",
    "        self.h_b = t.distributions.Normal(t.zeros(self.mean.size()).cuda(), t.exp(self.h_sigma_b) * weights_b)\n",
    "        \n",
    "        \n",
    "        k1 = t.distributions.kl_divergence(self.eps_w,self.h_w).sum()        \n",
    "        k2 = t.distributions.kl_divergence(self.eps_b,self.h_b).sum()        \n",
    "        return k1+k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedLayer(nn.Module):\n",
    "    def __init__(self, in_, dims, lambda_t, act=F.tanh):\n",
    "        nn.Module.__init__(self)    \n",
    "        self.lambda_t = lambda_t\n",
    "        self.dims = dims\n",
    "        self.layer = VarLayer(in_, max(dims), act=act)\n",
    "        \n",
    "        self.s = nn.Parameter(t.ones(len(dims), device='cuda')+ t.randn(len(dims), device='cuda')*.0)    \n",
    "        self.s_prior = nn.Parameter(t.ones(len(dims), device='cuda')+ t.randn(len(dims), device='cuda')*.0)\n",
    "        self.log_t = nn.Parameter(t.zeros(1, device='cuda'))           \n",
    "        self.t = t.exp(self.log_t)+0.2 \n",
    "        # мы добавляем шум в слои для уменьшения возможной подстройки одного слоя под другой\n",
    "        # в противном случае становится более вероятным случай, когда\n",
    "        # в первую очередь выбирается наиболее простая подмодель, а остальные \"достраиваются\"\n",
    "        # с небольшими значениями для структурных параметров\n",
    "        self.noises = [(t.randn(d).cuda()*1.0)  if d != 1 else t.randn(in_).cuda() for d in dims]\n",
    "        \n",
    "        \n",
    "    def forward(self, x, noise=0.0):        \n",
    "        self.t = t.exp(self.log_t)+0.2\n",
    "        self.gamma = t.distributions.RelaxedOneHotCategorical(self.t, logits=self.s)        \n",
    "        gamma = self.gamma.rsample()\n",
    "        var_result = self.layer(x, 1.0)\n",
    "        result = t.zeros(x.shape).cuda()\n",
    "        for i,d in enumerate(self.dims):\n",
    "            if d == 1:\n",
    "                result = result + (x+self.noises[i])*gamma[i] \n",
    "            else:\n",
    "                result[:,:d] =result[:,:d] +  (var_result[:,:d]+self.noises[i])*gamma[i]            \n",
    "        return result\n",
    "    \n",
    "    def kld_normal(self): \n",
    "        self.t = t.exp(self.log_t)+0.2\n",
    "        self.gamma = t.distributions.RelaxedOneHotCategorical(self.t, logits=self.s)        \n",
    "        sample = (self.gamma.rsample()+0.0001)\n",
    "        weights = t.zeros(self.layer.mean.size()).cuda()+0.0001\n",
    "        weights_b = t.zeros(self.layer.mean_b.size()).cuda()+0.0001\n",
    "        for i in range(len(self.dims)):\n",
    "            weights[:,:self.dims[i]]+=sample[i]\n",
    "            weights_b[:self.dims[i]]+=sample[i]\n",
    "            \n",
    "        \n",
    "        return self.layer.KLD(weights, weights_b)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def kld_structure(self):        \n",
    "        self.t = t.exp(self.log_t)+0.2\n",
    "        self.gamma_prior = t.distributions.RelaxedOneHotCategorical(self.lambda_t, logits=self.s_prior)\n",
    "        self.gamma = t.distributions.RelaxedOneHotCategorical(self.t, logits=self.s)\n",
    "        \n",
    "        sample = (self.gamma.rsample()+0.0001)\n",
    "        return self.gamma.log_prob(sample) - self.gamma_prior.log_prob(sample)\n",
    "        \n",
    "        \n",
    "    def KLD(self, w1=1.0, w2=1.0):\n",
    "        return self.kld_structure() *w1 + self.kld_normal() * w2\n",
    "\n",
    "        \n",
    "def nothing(x):\n",
    "    return x\n",
    "class ElboStructNet(nn.Module):\n",
    "    def __init__(self, dims,  layer_num, temp_tensor):\n",
    "        nn.Module.__init__(self)\n",
    "        layers = []\n",
    "        for l in range(layer_num):\n",
    "            layers.append(MixedLayer(input_dim, dims, temp_tensor )) \n",
    "        layers.append(VarLayer(input_dim, 10,act=nothing)) \n",
    "            \n",
    "        hyper_prior = 1.0\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.hyper_gamma = t.distributions.HalfNormal(t.ones(1,device='cuda')*hyper_prior)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    \n",
    "    def hyper_ll(self):\n",
    "        #https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\n",
    "        k = 0\n",
    "        for l in self.model:\n",
    "            if isinstance(l, VarLayer):                                 \n",
    "                    k+=self.hyper_gamma.log_prob(t.exp(l.h_sigma.view(-1))).sum()\n",
    "                    k+=self.hyper_gamma.log_prob(t.exp(l.h_sigma_b.view(-1))).sum()\n",
    "                \n",
    "        return k\n",
    "    \n",
    "\n",
    "    def structure_params(self):\n",
    "        result = []\n",
    "        for l in self.model:\n",
    "            if isinstance(l, MixedLayer):\n",
    "                result.append(l.s)\n",
    "                result.append(l.log_t)\n",
    "                result.append(l.s_prior)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def KLD(self, w1=1.0, w2 =1.0):\n",
    "        k = 0\n",
    "        for l in self.model:\n",
    "            if isinstance(l, MixedLayer):\n",
    "                k+=l.KLD( w1, w2)\n",
    "            if isinstance(l, VarLayer):\n",
    "                k+=l.KLD()\n",
    "        return k\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(model, loader, func = lambda x:x):\n",
    "    tp = 0\n",
    "    cases = 0\n",
    "    model.eval()\n",
    "    for x,y in loader: \n",
    "            x = func(x)\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            out = model(x).argmax(1)\n",
    "            tp+=(out==y).sum()\n",
    "            cases+=len(y)\n",
    "    model.train()\n",
    "    return  tp.cpu().numpy()*1.0/cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "t.manual_seed(random_seed)\n",
    "temp = t.ones(1).cuda()*0.2\n",
    "for trial in range(trials):\n",
    "    net = ElboStructNet(search_space, 4, temp)\n",
    "    struct = net.structure_params()\n",
    "    optimizer1 = optim.Adam([p for p in net.parameters() if p not in set(struct)], lr=0.001) \n",
    "    optimizer2 = optim.Adam(struct, lr=0.01) \n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()    \n",
    "    id=0\n",
    "    for epoch in range(n_epochs):       \n",
    "        for x,y in train_loader:\n",
    "            id+=1        \n",
    "            x = x.cuda()\n",
    "            y = y.cuda()            \n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "            loss = 0\n",
    "            out_loss = 0\n",
    "            kld =  t.zeros(1).cuda()             \n",
    "\n",
    "            for _ in range(5):\n",
    "                out = net(x)\n",
    "                out_loss += loss_fn(out, y)*len(train_idx)*1.0/5       \n",
    "            if epoch >= 0:\n",
    "                kld+=net.KLD()\n",
    "            hyper = -net.hyper_ll()\n",
    "\n",
    "            loss = (out_loss+kld + hyper)       \n",
    "            if id %10 == 0:\n",
    "                print (net.model[0].gamma.probs.data, net.model[0].gamma.temperature)\n",
    "                print ( out_loss.data, kld.data)\n",
    "                \n",
    "                print ('\\n')\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_value_(net.parameters(), 1.0)            \n",
    "            optimizer1.step()  \n",
    "            optimizer2.step()  \n",
    "\n",
    "        acc = test_acc(net, valid_loader)            \n",
    "        print ('Trial {0}. Epoch {1}. Acc: {2}'.format(trial, epoch, acc))\n",
    "    # в pickle нельзя сохранить распределения\n",
    "    # альтернатива: использовать встроенные средства сохранения в pytorch\n",
    "    for m in net.model:\n",
    "        try:\n",
    "            del m.gamma\n",
    "        except:\n",
    "            pass \n",
    "        try:\n",
    "            del m.lambda_t\n",
    "        except:\n",
    "            pass \n",
    "        try:\n",
    "            del m.gamma_prior\n",
    "        except:\n",
    "            pass \n",
    "        try:\n",
    "            del m.layer.eps_w\n",
    "        except:\n",
    "            pass \n",
    "        try:\n",
    "            del m.layer.eps_b\n",
    "        except:\n",
    "            pass \n",
    "    with open( 'old_proposed{0}.pckl'.format(trial), 'wb') as out:\n",
    "        pickle.dump(net, out)\n",
    "    del net\n",
    "    t.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'full_proposed{0}.pckl'.format(0), 'rb') as inp:\n",
    "        net = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение: Max Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        nn.Module.__init__(self)\n",
    "        self.bias = bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "    \n",
    "def elbonet_to_elbo(model):\n",
    "    var_layers = []\n",
    "    in_dim = input_dim\n",
    "    for mixed_layer in model.model[:-1]:  \n",
    "        \n",
    "        if isinstance(mixed_layer, MixedLayer):\n",
    "            best_layer_id = mixed_layer.s.argmax()\n",
    "            if mixed_layer.dims[best_layer_id] == 1:\n",
    "                continue\n",
    "            print (best_layer_id)\n",
    "            new_layer = VarLayer(in_dim, mixed_layer.dims[best_layer_id]).cuda()\n",
    "            \n",
    "            weights = t.zeros(mixed_layer.layer.mean.size()).cuda()+0.0001\n",
    "            weights_b = t.zeros(mixed_layer.layer.mean_b.size()).cuda()+0.0001\n",
    "            gamma = F.softmax(mixed_layer.s/mixed_layer.t)\n",
    "            for i in range(len(mixed_layer.dims)):\n",
    "                weights[:, :mixed_layer.dims[i]]+=gamma[i]\n",
    "                weights_b[:mixed_layer.dims[i]] += gamma[i]\n",
    "            weights = t.log(weights)\n",
    "            weights_b = t.log(weights_b)\n",
    "            \n",
    "            new_layer.mean.data *= 0\n",
    "            new_layer.mean.data += mixed_layer.layer.mean[:in_dim,:mixed_layer.dims[best_layer_id]]\n",
    "            new_layer.log_sigma.data *= 0\n",
    "            new_layer.log_sigma.data += mixed_layer.layer.log_sigma[:in_dim,:mixed_layer.dims[best_layer_id]]\n",
    "            \n",
    "            new_layer.mean_b.data *= 0\n",
    "            new_layer.mean_b.data += mixed_layer.layer.mean_b[:mixed_layer.dims[best_layer_id]]\n",
    "            new_layer.log_sigma_b.data *= 0\n",
    "            new_layer.log_sigma_b.data += mixed_layer.layer.log_sigma_b[:mixed_layer.dims[best_layer_id]]\n",
    "            \n",
    "            new_layer.h_sigma.data *= 0\n",
    "            new_layer.h_sigma.data += weights[:in_dim, :mixed_layer.dims[best_layer_id]]\n",
    "            new_layer.h_sigma.data += mixed_layer.layer.h_sigma[:in_dim, :mixed_layer.dims[best_layer_id]]\n",
    "            \n",
    "            new_layer.h_sigma_b.data *= 0\n",
    "            new_layer.h_sigma_b.data += weights_b[:mixed_layer.dims[best_layer_id]]\n",
    "            new_layer.h_sigma_b.data += mixed_layer.layer.h_sigma_b[:mixed_layer.dims[best_layer_id]]\n",
    "            \n",
    "            \n",
    "            var_layers.append(new_layer)\n",
    "            var_layers.append(AddLayer(mixed_layer.noises[best_layer_id][:mixed_layer.dims[best_layer_id]]))\n",
    "            \n",
    "            in_dim = mixed_layer.dims[best_layer_id]\n",
    "    \n",
    "    sublayer = model.model[-1] \n",
    "    out_ = 10\n",
    "    \n",
    "    new_layer = VarLayer(in_dim, out_, act=lambda x:x).cuda()\n",
    "    new_layer.mean.data *= 0\n",
    "    new_layer.mean.data += sublayer.mean[:in_dim]\n",
    "    new_layer.mean_b.data *= 0\n",
    "    new_layer.mean_b.data += sublayer.mean_b[:in_dim]\n",
    "    \n",
    "    new_layer.h_sigma.data *= 0\n",
    "    new_layer.h_sigma.data += sublayer.h_sigma[:in_dim]\n",
    "\n",
    "    new_layer.h_sigma_b.data *= 0\n",
    "    new_layer.h_sigma_b.data += sublayer.h_sigma_b[:in_dim]\n",
    "\n",
    "            \n",
    "    var_layers.append(new_layer)\n",
    "    return nn.Sequential(*var_layers)\n",
    "\n",
    "def elbonet_to_net(model):\n",
    "    var_layers = []\n",
    "    in_dim = input_dim\n",
    "    for mixed_layer in model.model[:-1]:        \n",
    "        if isinstance(mixed_layer, MixedLayer):\n",
    "            best_layer_id = mixed_layer.s.argmax()\n",
    "            if mixed_layer.dims[best_layer_id] == 1:\n",
    "                continue\n",
    "            print (best_layer_id)\n",
    "            new_layer = nn.Linear(in_dim, mixed_layer.dims[best_layer_id]).cuda()\n",
    "            \n",
    "            \n",
    "            new_layer.weight.data *= 0\n",
    "            new_layer.weight.data += mixed_layer.layer.mean.transpose(1,0)[:mixed_layer.dims[best_layer_id], :in_dim]\n",
    "            new_layer.bias.data *= 0\n",
    "            new_layer.bias.data += mixed_layer.layer.mean_b[:mixed_layer.dims[best_layer_id]]\n",
    "            var_layers.append(new_layer)\n",
    "            var_layers.append(nn.Tanh())\n",
    "            var_layers.append(AddLayer(mixed_layer.noises[best_layer_id][:mixed_layer.dims[best_layer_id]]))\n",
    "            \n",
    "            in_dim = mixed_layer.dims[best_layer_id]\n",
    "    \n",
    "    sublayer = model.model[-1] \n",
    "    out_ = 10\n",
    "    \n",
    "    new_layer =  nn.Linear(in_dim, out_).cuda()\n",
    "    new_layer.weight.data *= 0\n",
    "    new_layer.weight.data += sublayer.mean.transpose(1,0)[:, :in_dim]\n",
    "    new_layer.bias.data *= 0\n",
    "    new_layer.bias.data += sublayer.mean_b[:in_dim]\n",
    "    \n",
    "    \n",
    "    \n",
    "    var_layers.append(new_layer)\n",
    "    return nn.Sequential(*var_layers)\n",
    "\n",
    "#subnet = elbonet_to_net(net)\n",
    "#subnet = elbonet_to_elbo(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_nets = []\n",
    "for trial in range(trials):\n",
    "    with open( 'old_proposed{0}.pckl'.format(trial), 'rb') as inp:\n",
    "        net = pickle.load(inp)\n",
    "    subnet = elbonet_to_net(net)\n",
    "    optimizer1 = optim.Adam(subnet.parameters(), lr=0.001) # для параметров\n",
    "    loss_fn = nn.CrossEntropyLoss()    \n",
    "    id=0\n",
    "    for epoch in range(fine_tune_epochs):        \n",
    "        for x,y in train_loader:\n",
    "            id+=1\n",
    "\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()            \n",
    "            optimizer1.zero_grad()        \n",
    "            loss = 0\n",
    "            out_loss = 0\n",
    "            out = subnet(x)\n",
    "            out_loss += loss_fn(out, y)*len(train_idx)*1.0        \n",
    "            loss = (out_loss)       \n",
    "            if id %100 == 0:            \n",
    "                print (out_loss.data)\n",
    "                print ('\\n')\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_value_(net.parameters(), 1.0)            \n",
    "            optimizer1.step()  \n",
    "\n",
    "        acc = test_acc(subnet, valid_loader)            \n",
    "        print ('Trial {0}. Epoch {1}. Acc: {2}'.format(trial, epoch, acc))\n",
    "    tuned_nets.append(subnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./old_proposed_tuned.pckl', 'wb') as out:\n",
    "    pickle.dump(tuned_nets, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = {}\n",
    "pn = []\n",
    "for subnet in tuned_nets:    \n",
    "    num = 0\n",
    "    for p in subnet.parameters():\n",
    "    \n",
    "        if len(p.size())==1:\n",
    "            num+=p.size()[0]\n",
    "        elif len(p.size())==0:\n",
    "            num+=1\n",
    "        else:\n",
    "            num+=p.size()[1]*p.size()[0]\n",
    "    pn.append(num)\n",
    "\n",
    "stats['param number'] = pn\n",
    "stats['param number']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superposition_number(): \n",
    "    sn = []\n",
    "    for subnet in tuned_nets:\n",
    "        cnt = 0\n",
    "        for submodel in subnet:\n",
    "            print (submodel)\n",
    "            if len(list(submodel.parameters()))>0:\n",
    "                cnt+=1\n",
    "        sn.append(cnt)\n",
    "        \n",
    "    return sn\n",
    "stats['superposition number'] = get_superposition_number()\n",
    "stats['superposition number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "X = []\n",
    "Y = []\n",
    "Y_std = []\n",
    "accs = []\n",
    "for noise in np.linspace(0, 1.0, 10):\n",
    "    X.append(noise)\n",
    "    acc = []\n",
    "    for subnet in tuned_nets:\n",
    "                     \n",
    "        acc += [test_acc(subnet, test_loader, func = lambda x: x+t.randn(x.size())*noise)] \n",
    "    print (acc)\n",
    "    Y.append(np.mean(acc))\n",
    "    Y_std.append(np.std(acc))\n",
    "    accs.append(acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['noise'] = [X,Y,Y_std, accs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "X = []\n",
    "Y = []\n",
    "accs = []\n",
    "Y_std = []\n",
    "for noise in np.linspace(0, 0.1, 10):\n",
    "    X.append(noise)\n",
    "    acc = []\n",
    "    for subnet in tuned_nets:\n",
    "        m = subnet\n",
    "        m.eval()\n",
    "        old_params = []\n",
    "        for p in m.parameters():\n",
    "            old_params.append(p.data*1.0)\n",
    "\n",
    "        tp = 0        \n",
    "        for x,y in test_loader:\n",
    "\n",
    "            for p, o in zip(m.parameters(), old_params):                \n",
    "                n = t.randn(p.data.shape)*noise\n",
    "                n = n.cuda()                    \n",
    "                p.data = o + n\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            out = m(x).argmax(1)\n",
    "            tp+=(out==y).sum()\n",
    "            for p, o in zip(m.parameters(), old_params):                \n",
    "                p.data = o\n",
    "        acc.append(tp.cpu().numpy()*1.0/len(test_data))\n",
    "    print (acc)\n",
    "    accs.append(acc)\n",
    "    Y.append(np.mean(acc))\n",
    "    Y_std.append(np.std(acc))\n",
    "stats['params'] = [X,Y,Y_std, accs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./old_proposed_ml_stats.pckl', 'wb') as out:\n",
    "    pickle.dump(stats, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение: ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tuned_nets = []\n",
    "for trial in range(trials):\n",
    "    with open( 'old_proposed{0}.pckl'.format(trial), 'rb') as inp:\n",
    "        net = pickle.load(inp)\n",
    "    subnet = elbonet_to_elbo(net)\n",
    "    optimizer1 = optim.Adam(subnet.parameters(), lr=0.001) # для параметров\n",
    "    loss_fn = nn.CrossEntropyLoss()    \n",
    "    id=0\n",
    "    for epoch in range(fine_tune_epochs):        \n",
    "        for x,y in train_loader:\n",
    "            id+=1\n",
    "\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()            \n",
    "            optimizer1.zero_grad()        \n",
    "            loss = 0\n",
    "            out_loss = 0\n",
    "            out = subnet(x)\n",
    "            for _ in range(5):\n",
    "                out_loss += loss_fn(out, y)*len(train_idx)*1.0/5\n",
    "            k = 0\n",
    "            for m in subnet:\n",
    "                if isinstance(m, VarLayer):\n",
    "                    k+=m.KLD()\n",
    "            loss = (out_loss+k)       \n",
    "            if id %100 == 0:            \n",
    "                print (out_loss.data, k.data)\n",
    "                print ('\\n')\n",
    "\n",
    "            loss.backward()\n",
    "            clip_grad_value_(net.parameters(), 1.0)            \n",
    "            optimizer1.step()  \n",
    "\n",
    "        acc = test_acc(subnet, valid_loader)            \n",
    "        print ('Trial {0}. Epoch {1}. Acc: {2}'.format(trial, epoch, acc))\n",
    "    tuned_nets.append(subnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tuned_nets:\n",
    "   n[-1].act = nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./old_proposed_tuned_var.pckl', 'wb') as out:\n",
    "    pickle.dump(tuned_nets, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./old_proposed_tuned_var.pckl', 'rb') as inp:\n",
    "    tuned_nets = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "X = []\n",
    "Y = []\n",
    "Y_std = []\n",
    "accs = []\n",
    "for noise in np.linspace(0, 1.0, 10):\n",
    "    X.append(noise)\n",
    "    acc = []\n",
    "    for subnet in tuned_nets:\n",
    "                     \n",
    "        acc += [test_acc(subnet, test_loader, func = lambda x: x+t.randn(x.size())*noise)] \n",
    "    print (acc)\n",
    "    Y.append(np.mean(acc))\n",
    "    Y_std.append(np.std(acc))\n",
    "    accs.append(acc)\n",
    "stats['noise'] =[X,Y,Y_std, accs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "X = []\n",
    "Y = []\n",
    "accs = []\n",
    "Y_std = []\n",
    "for noise in np.linspace(0, 0.1, 10):\n",
    "    X.append(noise)\n",
    "    acc = []\n",
    "    for subnet in tuned_nets:\n",
    "        subnet.eval()\n",
    "        m = subnet\n",
    "\n",
    "        old_params = []\n",
    "        for p in m.parameters():\n",
    "            old_params.append(p.data*1.0)\n",
    "\n",
    "        tp = 0        \n",
    "        for x,y in test_loader:\n",
    "\n",
    "            for p, o in zip(m.parameters(), old_params):                \n",
    "                n = t.randn(p.data.shape)*noise\n",
    "                n = n.cuda()                    \n",
    "                p.data = o + n\n",
    "            \n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            out = m(x).argmax(1)\n",
    "            tp+=(out==y).sum()\n",
    "            for p, o in zip(m.parameters(), old_params):                \n",
    "                p.data = o\n",
    "        acc.append(tp.cpu().numpy()*1.0/len(test_data))\n",
    "    print (acc)\n",
    "    accs.append(acc)\n",
    "    Y.append(np.mean(acc))\n",
    "    Y_std.append(np.std(acc))\n",
    "stats['params'] = [X,Y,Y_std, accs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./old_proposed_var_stats.pckl', 'wb') as out:\n",
    "    pickle.dump(stats, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
