{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск структуры методом случайного поиска\n",
    "\n",
    "Модель - MLP с не более чем 4 скрытыми слоями (всего слоев не более 5, если считать softmax).\n",
    "\n",
    "Для каждого скрытого слоя существуют следующие варианты подмоделей:\n",
    " * тождественное отображение \n",
    " * 16 нейронов\n",
    " * 32 нейрона\n",
    " * 64 нейрона\n",
    " * 256 нейронов\n",
    " * 512 нейронов\n",
    " * 1024 нейрона\n",
    " \n",
    "Итого, пространство поиска составляет 6^4+6^3+6^2+6 = 1554 модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек, объявление функций и констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "%matplotlib inline\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 32*32*1 # размерность CIFAR, grayscale\n",
    "class_num = 10\n",
    "n_epochs =  30\n",
    "batch_size = 256\n",
    "random_seed = 42\n",
    "valid_size = 0.1 \n",
    "trials = 10 # количество повторений эксперимента\n",
    "series_num = 5 # количество попыток подобрать параметры в случайном поике\n",
    "search_space = [1, 16, 32, 64, 256, 512, 1024]  # '1' кодирует тождественное отображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10('./files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.mean(0).view(-1))\n",
    "                             ]))\n",
    "\n",
    "test_data = torchvision.datasets.CIFAR10('./files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                              (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                  torchvision.transforms.Lambda(lambda x: x.mean(0).view(-1))\n",
    "                             ]))\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=0, pin_memory=True )\n",
    "test_loader = t.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "valid_loader = t.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    _layers = []\n",
    "    x_in = input_dim\n",
    "    for l in layers:\n",
    "        if l ==1:\n",
    "            continue\n",
    "        else:\n",
    "            _layers.append(nn.Linear(x_in, l))\n",
    "            _layers.append( nn.Tanh())\n",
    "            x_in = l\n",
    "    _layers.append(nn.Linear(x_in, class_num))\n",
    "    return  nn.Sequential(*_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(model, loader, func = lambda x:x):\n",
    "    tp = 0\n",
    "    cases = 0\n",
    "    for x,y in loader: \n",
    "            x = func(x)\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            out = model(x).argmax(1)\n",
    "            tp+=(out==y).sum()\n",
    "            cases+=len(y)\n",
    "    return  tp.cpu().numpy()*1.0/cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "t.manual_seed(random_seed)\n",
    "rs = np.random.RandomState(random_seed)\n",
    "\n",
    "for trial in range(trials): \n",
    "    best = 0\n",
    "    models.append([])\n",
    "    for series in range(series_num):        \n",
    "        layers = []\n",
    "        for l in range(5):\n",
    "            layers.append(rs.choice(search_space))\n",
    "        print ('Network:', layers)\n",
    "        net = build_model(layers)    \n",
    "        net = net.cuda()\n",
    "        \n",
    "\n",
    "        optimizer = optim.Adam(net.parameters())    \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        for epoch in range(n_epochs):\n",
    "            for x,y in train_loader:    \n",
    "\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()                \n",
    "                out = net(x)\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(out, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()            \n",
    "            print ('Trial: {0}. Series: {1}. Epoch: {2}. '.format(trial, series, epoch))\n",
    "\n",
    "        acc = test_acc(net, valid_loader)\n",
    "        print ('Accuracy', acc)\n",
    "        if acc>best:\n",
    "            print ('New best model!')    \n",
    "            models[-1].append(net)\n",
    "            best = acc\n",
    "        else:\n",
    "            models[-1].append(models[-1][-1]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models_rs.pckl', 'wb') as out:\n",
    "    pickle.dump(models, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ моделей\n",
    "\n",
    "### Общая статистика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models_rs.pckl', 'rb') as inp:\n",
    "    models = pickle.load(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "pn = []\n",
    "for subnet in models:\n",
    "    subnet = subnet[-1]\n",
    "    num = 0\n",
    "    for p in subnet.parameters():\n",
    "    \n",
    "        if len(p.size())==1:\n",
    "            num+=p.size()[0]\n",
    "        elif len(p.size())==0:\n",
    "            num+=1\n",
    "        else:\n",
    "            num+=p.size()[1]*p.size()[0]\n",
    "    pn.append(num)\n",
    "\n",
    "stats['param number'] = pn\n",
    "stats['param number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superposition_number(): \n",
    "    sn = []\n",
    "    for subnet in models:\n",
    "        subnet = subnet[-1]\n",
    "        cnt = 0\n",
    "        for submodel in subnet:\n",
    "            \n",
    "            print (submodel)\n",
    "            if len(list(submodel.parameters()))>0:\n",
    "                cnt+=1\n",
    "        sn.append(cnt)\n",
    "        \n",
    "    return sn\n",
    "stats['superposition number'] = get_superposition_number()\n",
    "stats['superposition number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Качество при добавлении шума в выборку: гауссовский шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[1]\n",
    "for noise in np.linspace(0, 1.0, 4):\n",
    "    plt.title(noise)\n",
    "    plt.imshow(x[0].reshape(32,32).cpu().numpy() + np.random.randn(32, 32)*noise)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "X = []\n",
    "Y = []\n",
    "Y_std = []\n",
    "accs = []\n",
    "for noise in np.linspace(0, 1.0, 10):\n",
    "    X.append(noise)\n",
    "    acc = []\n",
    "    for subnet in models:\n",
    "        subnet = subnet[-1]             \n",
    "        acc += [test_acc(subnet, test_loader, func = lambda x: x+t.randn(x.size())*noise)] \n",
    "    print (acc)\n",
    "    Y.append(np.mean(acc))\n",
    "    Y_std.append(np.std(acc))\n",
    "    accs.append(acc)\n",
    "stats['noise'] = [X,Y,Y_std, accs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Качество при добавлении шума в параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t.manual_seed(random_seed)\n",
    "X = []\n",
    "Y = []\n",
    "accs = []\n",
    "Y_std = []\n",
    "for noise in np.linspace(0, 0.1, 10):\n",
    "    X.append(noise)\n",
    "    acc = []\n",
    "    for subnet in models:\n",
    "        m = subnet[-1]\n",
    "        m.eval()\n",
    "        old_params = []\n",
    "        for p in m.parameters():\n",
    "            old_params.append(p.data*1.0)\n",
    "\n",
    "        tp = 0        \n",
    "        for x,y in test_loader:\n",
    "\n",
    "            for p, o in zip(m.parameters(), old_params):                \n",
    "                n = t.randn(p.data.shape)*noise\n",
    "                n = n.cuda()                    \n",
    "                p.data = o + n\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            out = m(x).argmax(1)\n",
    "            tp+=(out==y).sum()\n",
    "            for p, o in zip(m.parameters(), old_params):                \n",
    "                p.data = o\n",
    "        acc.append(tp.cpu().numpy()*1.0/len(test_data))\n",
    "    print (acc)\n",
    "    accs.append(acc)\n",
    "    Y.append(np.mean(acc))\n",
    "    Y_std.append(np.std(acc))\n",
    "stats['params'] = [X,Y,Y_std, accs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./stats_rs.pckl', 'wb') as out:\n",
    "    pickle.dump(stats, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
